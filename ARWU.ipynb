{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARWU-web-crawler for Global Ranking of Academic Subjects\n",
    "\n",
    "target link:  \n",
    "- [Global Ranking of Academic Subjects link](https://www.shanghairanking.com/rankings/gras/2021)\n",
    "  \n",
    "- [Mathematics subject link](https://www.shanghairanking.com/rankings/gras/2021/RS0101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installed finish\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "print(\"installed finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.support.ui import Select\n",
    "# test for open driver\n",
    "option = Options()\n",
    "option.add_argument(\"--disable-notifications\")\n",
    "chrome = webdriver.Chrome('./chromedriver', options=option)\n",
    "\n",
    "url = \"http://www.shanghairanking.com/rankings/gras/2021\"\n",
    "chrome.get(url)\n",
    "soup = BeautifulSoup(chrome.page_source, 'html.parser')\n",
    "chrome.close()\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Sciences\n",
      "Engineering\n",
      "Life Sciences\n",
      "Medical Sciences\n",
      "Social Sciences\n",
      "Area numbers: 5\n"
     ]
    }
   ],
   "source": [
    "## get 51 subject name\n",
    "url = \"http://www.shanghairanking.com/rankings/gras/2021\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "area_list = []\n",
    "areas = soup.find_all(class_=\"subject-title\")\n",
    "\n",
    "for area in areas:\n",
    "    area_list.append(area.text)\n",
    "    print(area.text.lstrip().rstrip())\n",
    "print(\"Area numbers:\", len(area_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mathematics     : RS0101\n",
      "Physics         : RS0102\n",
      "Chemistry       : RS0103\n",
      "Earth Sciences  : RS0104\n",
      "Geography       : RS0105\n",
      "Ecology         : RS0106\n",
      "Oceanography    : RS0107\n"
     ]
    }
   ],
   "source": [
    "subject_dict = {}\n",
    "subjects = soup.find_all(class_=\"subj-link\")\n",
    "\n",
    "for subject in subjects:\n",
    "    link = subject[\"href\"].split(\"/\")[-1]\n",
    "    subject_dict[subject.text] = link\n",
    "    \n",
    "count = 0\n",
    "for key, value in subject_dict.items():\n",
    "    print(\"%-15s : %6s\" % (key, value))\n",
    "    if(count > 5):\n",
    "        break\n",
    "    else:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find limited page number\n",
    "def get_final_page_number(sub_num):\n",
    "    subject_url = \"http://www.shanghairanking.com/rankings/gras/2021/\" + sub_num\n",
    "    chrome = webdriver.Chrome('./chromedriver', options=option)\n",
    "    chrome.get(subject_url)\n",
    "\n",
    "    soup = BeautifulSoup(chrome.page_source, 'lxml')\n",
    "    flag = False\n",
    "    tags_a = soup.find_all(\"a\")\n",
    "    limited_page = 0\n",
    "    for tag in tags_a:\n",
    "        if(tag.text == \"•••\" and flag == False):\n",
    "            flag = True\n",
    "        elif(flag == True):\n",
    "            try:\n",
    "                limited_page = int(tag.text)\n",
    "            except:\n",
    "                print(\"ERROR: Error with convert page number to integer.\")\n",
    "            break\n",
    "    print(\"limit page:\", limited_page)\n",
    "    chrome.close()\n",
    "    return limited_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one page's infomation\n",
    "def get_page_info(soup, tag):\n",
    "    print(f\"{tag} \", end=\"\")\n",
    "    items = soup.find_all(\"tr\")\n",
    "    flag = False\n",
    "    lists = [[], [], [], []]\n",
    "\n",
    "    for row in items:\n",
    "        if(flag == False):\n",
    "            flag = True\n",
    "            continue\n",
    "        single_list = []\n",
    "        # score and ranking\n",
    "        for i in [0,3,4]:\n",
    "            try:\n",
    "                text = row.contents[i].text.lstrip().rstrip()\n",
    "                single_list.append(text)\n",
    "            except:\n",
    "                print(\"ERROR: score ranking error.\")\n",
    "                pass\n",
    "        # institution name\n",
    "        try:\n",
    "            text = row.find(class_=\"univ-name\").text.lstrip().rstrip()\n",
    "            single_list.append(text)\n",
    "        except:\n",
    "            try:\n",
    "                text = row.find(class_=\"univ-name-normal\").text.lstrip().rstrip()\n",
    "                single_list.append(text)\n",
    "            except:\n",
    "                print(\"ERROR: university name error.\")\n",
    "                pass\n",
    "        \n",
    "        if(len(single_list) == 4):\n",
    "            for i in range(4):\n",
    "                lists[i].append(single_list[i])\n",
    "        else:\n",
    "            print(single_list, \" row error.\")\n",
    "\n",
    "    return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in data list with result\n",
    "def list_append_all(result, list1, list2, list3, list4):\n",
    "    list1 += result[0]     # ranking\n",
    "    list2 += result[1]     # total score\n",
    "    list3[0] += result[2]  # item score\n",
    "    list4 += result[3]     # institution name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl a subject \n",
    "\n",
    "Crawl a subject ranking with 5 scores. Get 5 scores for each university in the same page, and then switch to next page to get another universities' different scores.\n",
    "\n",
    "sample output :   \n",
    "\\[Page:  1\\] Q1 CNCI IC TOP AWARD  \n",
    "\\[Page:  2\\] Q1 CNCI IC TOP AWARD  \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def crawl_subject(link):\n",
    "    subject_url = \"http://www.shanghairanking.com/rankings/gras/2021/\" + link\n",
    "    chrome = webdriver.Chrome('./chromedriver', options=option)\n",
    "    chrome.get(subject_url)\n",
    "\n",
    "    count = 0\n",
    "    limited_page = get_final_page_number(link)\n",
    "    univ_name = []\n",
    "    total_score = []\n",
    "    item_score = [[] for i in range(5)]\n",
    "    ranking = []\n",
    "\n",
    "    while(count < limited_page):\n",
    "        time.sleep(1)\n",
    "        print(\"[Page: %2d] \" % (count + 1), end=\"\")\n",
    "        # 5 score for a page\n",
    "        for index in range(5):\n",
    "            btns = chrome.find_elements_by_class_name(\"head-bg\", )\n",
    "            btns[-1].click()\n",
    "\n",
    "            links = chrome.find_elements_by_tag_name(\"li\")\n",
    "            # for 5 item score\n",
    "            btn_name_list = [\"Q1\", \"CNCI\", \"IC\", \"TOP\", \"AWARD\"]\n",
    "            for link in links:\n",
    "                if(link.text == btn_name_list[index]):\n",
    "                    link.click()\n",
    "                    soup = BeautifulSoup(chrome.page_source, 'lxml')\n",
    "                    result = get_page_info(soup, btn_name_list[index])\n",
    "                    # start: construct all lists' info\n",
    "                    if(index == 0):\n",
    "                        list_append_all(result, ranking, total_score, item_score, univ_name)\n",
    "                    # only fill in item score\n",
    "                    else:\n",
    "                        item_score[index] += result[2]\n",
    "                    time.sleep(2)\n",
    "                    break  # to click option button\n",
    "        # click next page\n",
    "        btns_next_page = chrome.find_elements_by_class_name(\"ant-pagination-item-link\", )\n",
    "        btns_next_page[-1].click()\n",
    "        count += 1\n",
    "        # scroll to top\n",
    "        js=\"var action=document.documentElement.scrollTop=0\"\n",
    "        chrome.execute_script(js)\n",
    "        time.sleep(1)\n",
    "        print()\n",
    "\n",
    "    chrome.close()\n",
    "    print(\"- finish -\")\n",
    "    return univ_name, total_score, item_score, ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function\n",
    "\n",
    "For each subject, crawl its ranking data and output to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mathematics RS0101\n",
      "limit page: 17\n",
      "[Page:  1] Q1 CNCI IC TOP AWARD \n",
      "[Page:  2] Q1 CNCI IC TOP AWARD \n",
      "[Page:  3] Q1 CNCI IC TOP AWARD \n",
      "[Page:  4] Q1 CNCI IC TOP AWARD \n",
      "[Page:  5] Q1 CNCI IC TOP AWARD \n",
      "[Page:  6] Q1 CNCI IC TOP AWARD \n",
      "[Page:  7] Q1 CNCI IC TOP AWARD \n",
      "[Page:  8] Q1 CNCI IC TOP AWARD \n",
      "[Page:  9] Q1 CNCI IC TOP AWARD \n",
      "[Page: 10] Q1 CNCI IC TOP AWARD \n",
      "[Page: 11] Q1 CNCI IC TOP AWARD \n",
      "[Page: 12] Q1 CNCI IC TOP AWARD \n",
      "[Page: 13] Q1 CNCI IC TOP AWARD \n",
      "[Page: 14] Q1 CNCI IC TOP AWARD \n",
      "[Page: 15] Q1 CNCI IC TOP AWARD \n",
      "[Page: 16] Q1 CNCI IC TOP AWARD \n",
      "[Page: 17] Q1 CNCI IC TOP AWARD \n",
      "- finish -\n",
      "[ write finish ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "for subject, link in subject_dict.items():\n",
    "    print(subject, link)\n",
    "    univ_name, total_score, item_score, ranking = crawl_subject(link)\n",
    "    \n",
    "    # write file\n",
    "    df = pd.DataFrame()\n",
    "    df[\"Year\"] = [\"2021\" for i in range(len(ranking))]\n",
    "    # df[\"Field\"] = [\"\" for i in range(len(ranking))]\n",
    "    # df[\"Subject\"] = [subject for i in range(len(ranking))]\n",
    "    df[\"World Rank\"] = ranking\n",
    "    df[\"Institution\"] = univ_name\n",
    "    df[\"Total Score\"] = total_score\n",
    "    \n",
    "    name_list = [\"Q1\", \"CNCI\", \"IC\", \"TOP\", \"AWARD\"]\n",
    "    for index in range(len(name_list)):\n",
    "        df[name_list[index]] = item_score[index]\n",
    "        \n",
    "    df.to_csv(f\"ARWU-subject/{subject}.csv\", index=False)\n",
    "    print(\"[ write finish ]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
